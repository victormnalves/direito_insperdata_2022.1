{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ementa = list()\n",
    "processo = list()\n",
    "numero_processo = list()\n",
    "tipo = list()\n",
    "agencia_envolvida = list()\n",
    "relator = list()\n",
    "turma = list()\n",
    "data_julgamento = list()\n",
    "data_publicacao = list()\n",
    "agencia_envolvida = list()\n",
    "\n",
    "agencias = ['ANATEL', 'ANVISA', 'ANEEL', 'ANAC','ANTT', 'ANS', 'ANTAQ', 'ANP']\n",
    "for agencia in agencias:\n",
    "    path = 'D:/papers/direito/chromedriver_win32'\n",
    "    final_path = 'D:/papers/direito/scrapping'\n",
    "\n",
    "    chrome_options = Options()\n",
    "    #chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--window-size=1240,1080\") # define o tamanho da janela a ser aberta\n",
    "    driver = webdriver.Chrome(executable_path = os.path.join(path,'chromedriver.exe'), options = chrome_options) # o path do chromedriver\n",
    "    url = 'https://scon.stj.jus.br/SCON/pesquisar.jsp'\n",
    "    driver.get(url)\n",
    "    \n",
    "    pesquisa = driver.find_elements(by=By.XPATH, value='//*[@id=\"pesquisaLivre\"]')\n",
    "    #time.sleep(5)\n",
    "    pesquisa[0].send_keys(f'\"Recurso Especial\" {agencia} Nao embargo nao agravo') # buscando os acórdãos desejados\n",
    "\n",
    "    pesquisa_botao = driver.find_element(by=By.XPATH, value='//*[@id=\"frmConsulta\"]/div[3]/div[2]/div[2]/div/div/button') # salvando o botão de clique de busca\n",
    "    #time.sleep(3)\n",
    "    pesquisa_botao.click() # clicando na busca\n",
    "\n",
    "    for pagina in range(20):\n",
    "        \n",
    "        html_source = driver.page_source\n",
    "        soup = BeautifulSoup(html_source,\"html.parser\") # salvando a página encontrada pela busca\n",
    "\n",
    "        aumentar_acordaos = Select (driver.find_element(by=By.XPATH, value='//*[@id=\"qtdDocsPagina\"]')) # expandindo o número de acórdãos por página\n",
    "        aumentar_acordaos.select_by_value('50')\n",
    "        \n",
    "        simplificaco_botao = driver.find_element(by=By.XPATH, value='//*[@id=\"tp_vis_lista_resumida\"]/span') # esse botão simplificará a página, facilitando a busca pelo texto da ementa\n",
    "        #time.sleep(3)\n",
    "        simplificaco_botao.click()\n",
    "    \n",
    "        html_source = driver.page_source\n",
    "        soup = BeautifulSoup(html_source,\"html.parser\") # salvando a página simplificada\n",
    "\n",
    "        for i in range(0,len(soup.find_all('div',{'class':'row itemlistadocumentos p-2'}))): # obtendo a ementa\n",
    "            ementa.append(soup.find_all('div', {'class':'clsEmentaCompleta'})[i].get_text().replace('\\n','').replace('\\t',''))\n",
    "\n",
    "        completo_botao = driver.find_element(by=By.XPATH, value='//*[@id=\"tp_vis_documento_completo\"]/span') # esse botão volta a página à sua forma inicial, facilitando a busca por outros elementos\n",
    "        #time.sleep(3)\n",
    "        completo_botao.click()\n",
    "\n",
    "        html_source = driver.page_source\n",
    "        soup = BeautifulSoup(html_source,\"html.parser\") # salvando a página completa\n",
    "\n",
    "        for i in range(0, len(soup.find_all('div',{'class':'documento'}))): \n",
    "            agencia_envolvida.append(agencia) # salvando a agência que está sendo buscada\n",
    "            \n",
    "        for i in range(0,len(soup.find_all('div',{'class':'col-md-6 col-sm-12'})),2):\n",
    "            tipo.append(re.search('(?<=Processo)\\D{2,4}(?=\\d{0,})', re.sub('\\s+', '', soup.find_all('div',{'class':'col-md-6 col-sm-12'})[0].get_text()))[0]) # tipo do processo\n",
    "            processo.append(re.search('\\d{3,}\\s\\/\\s\\w\\w', soup.find_all('div',{'class':'col-md-6 col-sm-12'})[i].get_text())[0]) # salvando o elemento com detalhes de identificação do processo\n",
    "            numero_processo.append(re.sub('\\/','',re.sub('-', '', re.search('\\d{4,}\\/\\d{1,}\\-\\d{1,}', soup.find_all('div',{'class':'col-md-6 col-sm-12'})[i].get_text())[0])))\n",
    "\n",
    "        for i in range(0,len(soup.find_all('div',{'class':'col-md-3 col-sm-12'})),4): # esse elemento é uma lista de quatro elementos semelhantes, então o contador pulará de quatro em quatro\n",
    "            relator.append(re.subn('Relator[a]?', '', re.subn('\\\\n','',soup.find_all('div',{'class':'col-md-3 col-sm-12'})[0].get_text())[0])[0]) # salvando o relator\n",
    "            turma.append(re.search('(?<=\\\\n\\\\nÓrgão Julgador\\\\n).*(?=\\\\n\\\\n)',soup.find_all('div',{'class':'col-md-3 col-sm-12'})[i + 1].get_text())[0]) # salvando a turma\n",
    "            data_julgamento.append(re.search('\\d\\d\\/\\d\\d\\/\\d\\d\\d\\d',soup.find_all('div',{'class':'col-md-3 col-sm-12'})[i + 2].get_text())[0]) # salvando a data\n",
    "            data_publicacao.append(re.search('\\d\\d\\/\\d\\d\\/\\d\\d\\d\\d',soup.find_all('div',{'class':'col-md-3 col-sm-12'})[i + 3].get_text())[0]) # salvando a data\n",
    "        \n",
    "        try:\n",
    "            if pagina == 0:\n",
    "                #time.sleep(3)\n",
    "                driver.find_elements(by=By.XPATH, value='//*[@id=\"navegacao\"]/div[2]/a[1]/span')[0].click() # clique para a próxima página quando se está na primeira página\n",
    "            else:\n",
    "                #time.sleep(3)\n",
    "                driver.find_elements(by=By.XPATH, value='//*[@id=\"navegacao\"]/div[2]/a[3]/span')[0].click() # clique para a próxima página quando não se está na primeira página\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "dados1 = {'numero_processo': numero_processo,\n",
    "    'processo': processo,\n",
    "    'tipo_processo': tipo,\n",
    "    'ministro_relator': relator,\n",
    "    'orgao_julgador': turma,\n",
    "    'agencia': agencia_envolvida,\n",
    "    'data_julgamento': data_julgamento,\n",
    "    'data_publicacao': data_publicacao,\n",
    "    'ementa': ementa\n",
    "} # salvando todas as listas obtidas num dicionário\n",
    "\n",
    "df1 = pd.DataFrame.from_dict(dados1).drop_duplicates() # transformando o dicionário em um dataframe\n",
    "df1 = df1[df1.tipo_processo == 'REsp'] \n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorrente = []\n",
    "recorrido = []\n",
    "numero_processo_recorrente = []\n",
    "numero_processo_recorrido = []\n",
    "\n",
    "for processo in range(0,len(df1.numero_processo)):\n",
    "    print(processo)\n",
    "    path = 'D:/papers/direito/chromedriver_win32'\n",
    "    final_path = 'D:/papers/direito/scrapping'\n",
    "    chrome_options = Options()\n",
    "    #chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--window-size=1240,1080\") # define o tamanho da janela a ser aberta\n",
    "    driver = webdriver.Chrome(executable_path = os.path.join(path,'chromedriver.exe'), options = chrome_options) # o path do chromedriver\n",
    "    url = f\"https://processo.stj.jus.br/processo/pesquisa/?src=1.1.3&aplicacao=processos.ea&tipoPesquisa=tipoPesquisaGenerica&num_registro={df1['numero_processo'][processo]}\"\n",
    "    driver.get(url)\n",
    "\n",
    "    html_source = driver.page_source\n",
    "    soup = BeautifulSoup(html_source,\"html.parser\") # salvando a página encontrada pela busca\n",
    "    \n",
    "    for elemento in range(0,len(soup.find_all('span',{'class':'classSpanDetalhesLabel'}))):\n",
    "        texto = soup.find_all('span',{'class':'classSpanDetalhesLabel'})[elemento].get_text()\n",
    "        if texto == 'RECORRENTE :' or texto == 'EMBARGANTE:' or texto == 'SUSCITANTE:':\n",
    "            recorrente.append(re.sub('\\n','',soup.find_all('span',{'class':'classSpanDetalhesTexto'})[elemento].get_text()))\n",
    "            numero_processo_recorrente.append(df1['numero_processo'][processo])\n",
    "        if texto == 'RECORRIDO :' or texto == 'EMBARGADO :' or texto == 'SUSCITADO :':\n",
    "            recorrido.append(re.sub('\\n','',soup.find_all('span',{'class':'classSpanDetalhesTexto'})[elemento].get_text()))\n",
    "            numero_processo_recorrido.append(df1['numero_processo'][processo])\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "dados_recorrente = {'numero_processo': numero_processo_recorrente,\n",
    "    'recorrentes': recorrente\n",
    "}\n",
    "df_recorrente = pd.DataFrame.from_dict(dados_recorrente)\n",
    "\n",
    "dados_recorrido = {'numero_processo': numero_processo_recorrido,\n",
    "    'recorridos': recorrido\n",
    "} \n",
    "df_recorrido = pd.DataFrame.from_dict(dados_recorrido)\n",
    "\n",
    "df2 = df_recorrente.merge(df_recorrido, on='numero_processo', how='outer')\n",
    "\n",
    "df2 = pd.DataFrame.from_dict(dados2).drop_duplicates()\n",
    "df3 = df1.merge(df2, on='numero_processo', how='left')\n",
    "df3 = df3[[\"processo\",\"numero_processo\",\"tipo_processo\",\"ministro_relator\",\"ministro_relator_acordao\",\"orgao_julgador\",\"recorrentes\",\"recorridos\",\"data_julgamento\",\"data_publicacao\",\"ementa\"]]\n",
    "df3.to_excel('D:/papers/direito/scrapping/base.xlsx', index=False)\n",
    "df3"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "522658195921e7d96858f4e11ee8eef2ad0ae779a448c3f502a89235e2a5a231"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
